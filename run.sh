export NEPTUNE_PROJECT="thanhhau097/lecr"
export NEPTUNE_API_TOKEN="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlMTRjM2ExOC1lYTA5LTQwODctODMxNi1jZjEzMjdlMjkxYTgifQ=="

python train.py --do_train --do_eval --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-3 --warmup_ratio 0.01 --lr_scheduler_type cosine --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --logging_strategy steps --logging_steps 200 --save_total_limit 2 --load_best_model_at_end True --optim adamw_torch --weight_decay 1e-2 --num_train_epochs 1000 --metric_for_best_model eval_kendalltau --greater_is_better=True --dataloader_num_workers=16 --max_grad_norm=1.0 --data_type layout --source xla --search default --overwrite_output_dir=True --output_dir ./outputs_layout_xla_default/ --load_best_model_at_end True --hidden_channels 32,64,64,84 --report_to neptune
python train.py --do_train --do_eval --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-3 --warmup_ratio 0.01 --lr_scheduler_type cosine --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --logging_strategy steps --logging_steps 200 --save_total_limit 2 --load_best_model_at_end True --optim adamw_torch --weight_decay 1e-2 --num_train_epochs 1000 --metric_for_best_model eval_kendalltau --greater_is_better=True --dataloader_num_workers=16 --max_grad_norm=1.0 --data_type layout --source xla --search random --overwrite_output_dir=True --output_dir ./outputs_layout_xla_random/ --load_best_model_at_end True --hidden_channels 32,64,64,84 --report_to neptune
# python train.py --do_train --do_eval --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-3 --warmup_ratio 0.01 --lr_scheduler_type cosine --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --logging_strategy steps --logging_steps 200 --save_total_limit 2 --load_best_model_at_end True --optim adamw_torch --weight_decay 1e-2 --num_train_epochs 1000 --metric_for_best_model eval_kendalltau --greater_is_better=True --dataloader_num_workers=8 --max_grad_norm=1.0 --data_type layout --source nlp --search default --overwrite_output_dir=True --output_dir ./outputs_layout_nlp_default/ --load_best_model_at_end True --hidden_channels 32,64,64,84 --report_to neptune
# python train.py --do_train --do_eval --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-3 --warmup_ratio 0.01 --lr_scheduler_type cosine --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --logging_strategy steps --logging_steps 200 --save_total_limit 2 --load_best_model_at_end True --optim adamw_torch --weight_decay 1e-2 --num_train_epochs 1000 --metric_for_best_model eval_kendalltau --greater_is_better=True --dataloader_num_workers=8 --max_grad_norm=1.0 --data_type layout --source nlp --search random --overwrite_output_dir=True --output_dir ./outputs_layout_nlp_random/ --load_best_model_at_end True --hidden_channels 32,64,64,84 --report_to neptune
python train.py --do_train --do_eval --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-4 --warmup_ratio 0.01 --lr_scheduler_type cosine --save_strategy epoch --evaluation_strategy epoch --logging_strategy steps --logging_steps 200 --save_total_limit 2 --load_best_model_at_end True --optim adamw_torch --weight_decay 1e-2 --num_train_epochs 50 --metric_for_best_model eval_score_tile_mean --greater_is_better=True --dataloader_num_workers=8 --max_grad_norm=1.0 --overwrite_output_dir=True --output_dir ./outputs_tile/ --report_to neptune --report_to neptune


# inference
python train.py --do_predict --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --dataloader_num_workers=8 --data_type layout --source xla --search default --overwrite_output_dir=True --output_dir ./outputs_layout_xla_default_inference/ --hidden_channels 32,64,64,84 --report_to none --resume ./outputs_layout_xla_default/pytorch_model.bin
python train.py --do_predict --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --dataloader_num_workers=8 --data_type layout --source xla --search random --overwrite_output_dir=True --output_dir ./outputs_layout_xla_random_inference/ --hidden_channels 32,64,64,84 --report_to none --resume ./outputs_layout_xla_random/pytorch_model.bin
# python train.py --do_predict --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --dataloader_num_workers=8 --max_grad_norm=1.0 --data_type layout --source nlp --search default --overwrite_output_dir=True --output_dir ./outputs_layout_nlp_default_inference/ --load_best_model_at_end True --hidden_channels 32,64,64,84 --report_to none
# python train.py --do_predict --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --dataloader_num_workers=8 --max_grad_norm=1.0 --data_type layout --source nlp --search random --overwrite_output_dir=True --output_dir ./outputs_layout_nlp_random_inference/ --load_best_model_at_end True --hidden_channels 32,64,64,84 --report_to none
python train.py --do_predict --per_device_train_batch_size 1 --per_device_eval_batch_size 1  --dataloader_num_workers=8 --data_type tile --source xla --overwrite_output_dir=True --output_dir ./outputs_tile_inference/ --report_to none --resume ./outputs_tile/pytorch_model.bin
