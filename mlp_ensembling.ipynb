{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp torch model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# import torch dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, df, split=\"train\", chunk_size=128):\n",
    "        self.split = split\n",
    "        self.chunk_size = chunk_size\n",
    "        self.df = df\n",
    "        if split != \"train\":\n",
    "            new_df = []\n",
    "            for i, row in self.df.iterrows():\n",
    "                # split features and target into chunks\n",
    "                features = row[\"features\"]\n",
    "                targets = row[\"target\"]\n",
    "                n_chunks = len(features) // chunk_size\n",
    "                # n_chunks = int(np.ceil(n_chunks))\n",
    "                feature_chunks = np.array_split(features, n_chunks)\n",
    "                target_chunks = np.array_split(targets, n_chunks)\n",
    "\n",
    "                for feature, target in zip(feature_chunks, target_chunks):\n",
    "                    new_df.append(\n",
    "                        {\n",
    "                            \"file\": row[\"file\"],\n",
    "                            \"features\": feature,\n",
    "                            \"target\": target,\n",
    "                        }\n",
    "                    )\n",
    "            self.df = pd.DataFrame.from_dict(new_df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "\n",
    "        X = item[\"features\"]\n",
    "        y = item[\"target\"]\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            # random choose chunk_size indices\n",
    "            indices = random.sample(list(range(len(X))), self.chunk_size)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "        return {\n",
    "            \"file\": item[\"file\"],\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X = torch.stack([torch.tensor(x[\"X\"], dtype=torch.float32) for x in batch])\n",
    "    y = torch.tensor([x[\"y\"] for x in batch], dtype=torch.float32)\n",
    "    return X, y, batch[0][\"file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(64, 16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.classifier = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet50.4x4.fp16.npz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<01:57,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 0 \t Train loss: 0.3582819551229477 \t Test loss: 0.22334577420423196 \t Average tau: 0.5958446290200594\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 1 \t Train loss: 0.3307611793279648 \t Test loss: 0.22297775641430256 \t Average tau: 0.5959503618594728\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:00<01:58,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 2 \t Train loss: 0.2865614891052246 \t Test loss: 0.2237213375263436 \t Average tau: 0.5961204888621219\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 682/1000 [01:21<00:36,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 680 \t Train loss: 0.2689455027381579 \t Test loss: 0.24727154228576395 \t Average tau: 0.6434016670802672\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 704/1000 [01:23<00:33,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 702 \t Train loss: 0.2539753665526708 \t Test loss: 0.25347705774529034 \t Average tau: 0.6543364989149754\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 703 \t Train loss: 0.25937436024347943 \t Test loss: 0.24723460196062577 \t Average tau: 0.6685551172107206\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:01<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6685551172107206 with chunk size 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<01:16, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 0 \t Train loss: 0.35828570524851483 \t Test loss: 0.2226115188428334 \t Average tau: 0.5929406350861406\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 1 \t Train loss: 0.3210902710755666 \t Test loss: 0.224845324243818 \t Average tau: 0.593336859655092\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 2 \t Train loss: 0.3062420388062795 \t Test loss: 0.22458847221874056 \t Average tau: 0.5938430074777326\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:00<01:16, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 3 \t Train loss: 0.290676087141037 \t Test loss: 0.22475089771406992 \t Average tau: 0.5941051696991585\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 4 \t Train loss: 0.3041311129927635 \t Test loss: 0.22444704884574526 \t Average tau: 0.5942604411980086\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [00:00<01:16, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 5 \t Train loss: 0.2877661883831024 \t Test loss: 0.22200242607366472 \t Average tau: 0.5947090100591115\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [00:00<01:16, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 6 \t Train loss: 0.292085940639178 \t Test loss: 0.22173188910597846 \t Average tau: 0.5947317497729152\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 8 \t Train loss: 0.2947072933117549 \t Test loss: 0.22274166274638402 \t Average tau: 0.5951768879682547\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [00:00<01:16, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 9 \t Train loss: 0.28381893038749695 \t Test loss: 0.22229752512205214 \t Average tau: 0.5952119262692984\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 678/1000 [00:52<00:24, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 678 \t Train loss: 0.25516920288403827 \t Test loss: 0.25358633129369645 \t Average tau: 0.62070914498156\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 683/1000 [00:53<00:59,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 682 \t Train loss: 0.2601294244329135 \t Test loss: 0.233894767505782 \t Average tau: 0.665873005317078\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 706/1000 [00:57<00:26, 10.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 703 \t Train loss: 0.2779843658208847 \t Test loss: 0.2482170377458845 \t Average tau: 0.6724960706696206\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:20<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6724960706696206 with chunk size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<00:58, 17.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 0 \t Train loss: 0.34731827427943546 \t Test loss: 0.22234828472137452 \t Average tau: 0.595575918039249\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 2 \t Train loss: 0.2982201874256134 \t Test loss: 0.2242656856775284 \t Average tau: 0.5958582026175162\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 640/1000 [00:41<00:21, 17.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 636 \t Train loss: 0.26901625593503314 \t Test loss: 0.23280747085809708 \t Average tau: 0.6241021981465269\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 648/1000 [00:42<00:23, 15.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 645 \t Train loss: 0.27192391951878864 \t Test loss: 0.22994683235883712 \t Average tau: 0.6242464921836248\n",
      "                --------------------------------------------------------\n",
      "            \n",
      "\n",
      "                Epoch: 647 \t Train loss: 0.2654834936062495 \t Test loss: 0.24934960454702376 \t Average tau: 0.6584910242572629\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 656/1000 [00:42<00:20, 16.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 653 \t Train loss: 0.27116520206133526 \t Test loss: 0.24884019494056703 \t Average tau: 0.6823647768518786\n",
      "                --------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:03<00:00, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6823647768518786 with chunk size 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from loss import PairwiseHingeLoss\n",
    "from scipy.stats import kendalltau\n",
    "from collections import defaultdict\n",
    "\n",
    "# example data\n",
    "INPUT_DIM = 660\n",
    "\n",
    "output_dir = \"/home/thanh/google_fast_or_slow/outputs_embeddings/outputs_embeddings\"\n",
    "\n",
    "all_files = [os.path.join(output_dir, file) for file in os.listdir(output_dir) if \"_valid_\" in file]\n",
    "file_to_features = defaultdict(list)\n",
    "file_to_target = defaultdict(list)\n",
    "for file in all_files:\n",
    "    data = np.load(file, allow_pickle=True).item()\n",
    "    for graph, emb, target in zip(data[\"files\"], data[\"embeddings\"], data[\"gts\"]):\n",
    "        file_to_features[graph].append(emb)\n",
    "        file_to_target[graph] = target\n",
    "\n",
    "all_graphs = list(file_to_features.keys())\n",
    "test_index = np.random.choice(len(all_graphs), 1)[0]\n",
    "test_graphs = [all_graphs[test_index]]\n",
    "train_graphs = [graph for graph in all_graphs if graph not in test_graphs]\n",
    "\n",
    "# train_graphs = all_graphs[:-1]\n",
    "# test_graphs = all_graphs[-1:]\n",
    "\n",
    "print(test_graphs)\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"file\": train_graphs,\n",
    "        \"features\": [np.concatenate(file_to_features[file], axis=1) for file in train_graphs],\n",
    "        \"target\": [file_to_target[file] for file in train_graphs],\n",
    "    }\n",
    ")\n",
    "\n",
    "test_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"file\": test_graphs,\n",
    "        \"features\": [np.concatenate(file_to_features[file], axis=1) for file in test_graphs],\n",
    "        \"target\": [file_to_target[file] for file in test_graphs],\n",
    "    }\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for CHUNK_SIZE in [128, 256, 512]:\n",
    "    train_dataset = EnsembleDataset(train_df, \"train\", CHUNK_SIZE)\n",
    "    test_dataset = EnsembleDataset(test_df, \"test\", CHUNK_SIZE)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = Model(INPUT_DIM).to(device)\n",
    "\n",
    "    n_epochs = 1000\n",
    "    lr = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    criterion = PairwiseHingeLoss()\n",
    "\n",
    "    best_score = 0\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y, file = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y, n=torch.tensor([y_pred.shape[1]], device=y_pred.device))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        file_preds = defaultdict(list)\n",
    "        file_gts = defaultdict(list)\n",
    "        test_loss = 0\n",
    "        for batch in val_dataloader:\n",
    "            x, y, file = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y, n=torch.tensor([y_pred.shape[1]], device=y_pred.device))\n",
    "            test_loss += loss.item()\n",
    "            file_preds[file].extend(y_pred[0].detach().cpu().numpy().tolist())\n",
    "            file_gts[file].extend(y[0].detach().cpu().numpy().tolist())\n",
    "        \n",
    "        # calculate metric for each file\n",
    "        all_taus = []\n",
    "        for file in file_preds:\n",
    "            tau, _ = kendalltau(file_preds[file], np.array(file_gts[file]).reshape(-1))\n",
    "            # print(f\"File {file} has tau {tau}\")\n",
    "            all_taus.append(tau)\n",
    "\n",
    "        if np.mean(all_taus) > best_score:\n",
    "            best_score = np.mean(all_taus)\n",
    "            print(f\"\"\"\n",
    "                Epoch: {epoch} \\t Train loss: {train_loss / len(train_dataloader)} \\t Test loss: {test_loss / len(val_dataloader)} \\t Average tau: {np.mean(all_taus)}\n",
    "                --------------------------------------------------------\n",
    "            \"\"\")\n",
    "            torch.save(model.state_dict(), f\"mlp_ensemble_{CHUNK_SIZE}.pt\")\n",
    "\n",
    "    print(f\"Best score: {best_score} with chunk size {CHUNK_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "INFERENCE_CHUNK_SIZE = 512\n",
    "model.load_state_dict(torch.load(f\"mlp_ensemble_{INFERENCE_CHUNK_SIZE}.pt\"))\n",
    "output_dir = \"/home/thanh/google_fast_or_slow/outputs_embeddings/outputs_embeddings\"\n",
    "\n",
    "all_files = [os.path.join(output_dir, file) for file in os.listdir(output_dir) if \"_test_\" in file]\n",
    "file_to_features = defaultdict(list)\n",
    "file_to_target = defaultdict(list)\n",
    "\n",
    "for file in all_files:\n",
    "    data = np.load(file, allow_pickle=True).item()\n",
    "    for graph, emb, target in zip(data[\"files\"], data[\"embeddings\"], data[\"gts\"]):\n",
    "        file_to_features[graph].append(emb)\n",
    "        file_to_target[graph] = target\n",
    "\n",
    "all_graphs = list(file_to_features.keys())\n",
    "\n",
    "inference_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"file\": all_graphs,\n",
    "        \"features\": [np.concatenate(file_to_features[file], axis=1) for file in all_graphs],\n",
    "        \"target\": [file_to_target[file] for file in all_graphs],\n",
    "    }\n",
    ")\n",
    "\n",
    "inference_dataset = EnsembleDataset(inference_df, \"test\", INFERENCE_CHUNK_SIZE)\n",
    "inference_dataloader = DataLoader(inference_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "file_preds = defaultdict(list)\n",
    "\n",
    "for batch in inference_dataloader:\n",
    "    x, y, file = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_pred = model(x)\n",
    "    file_preds[file].extend(y_pred[0].detach().cpu().numpy().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TopConfigs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>layout:xla:default:05ae41e26dd3c4c06390371a042...</td>\n",
       "      <td>730;585;874;498;787;197;709;345;244;232;323;36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>layout:xla:default:3e7156ac468dfb75cf5c9615e1e...</td>\n",
       "      <td>93;794;541;178;234;39;853;948;275;357;245;75;1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>layout:xla:default:5335ed13823b0a518ee3c79ba44...</td>\n",
       "      <td>757;395;337;522;916;178;958;779;179;540;687;12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>layout:xla:default:937ee0eb0d5d6151b7b8252933b...</td>\n",
       "      <td>85;132;93;757;107;163;991;145;850;786;125;51;8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>layout:xla:default:cd708819d3f5103afd6460b15e7...</td>\n",
       "      <td>366;245;153;310;711;848;111;313;256;689;232;33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>layout:xla:default:db59a991b7c607634f13570d52c...</td>\n",
       "      <td>425;411;729;620;367;286;571;788;379;59;899;658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>layout:xla:default:e8a3a1401b5e79f66d7037e424f...</td>\n",
       "      <td>428;310;625;920;731;216;619;217;153;879;52;415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>layout:xla:default:fbaa8bb6a1aed9988281085c910...</td>\n",
       "      <td>668;224;881;242;222;546;726;924;658;859;984;16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ID  \\\n",
       "0  layout:xla:default:05ae41e26dd3c4c06390371a042...   \n",
       "1  layout:xla:default:3e7156ac468dfb75cf5c9615e1e...   \n",
       "2  layout:xla:default:5335ed13823b0a518ee3c79ba44...   \n",
       "3  layout:xla:default:937ee0eb0d5d6151b7b8252933b...   \n",
       "4  layout:xla:default:cd708819d3f5103afd6460b15e7...   \n",
       "5  layout:xla:default:db59a991b7c607634f13570d52c...   \n",
       "6  layout:xla:default:e8a3a1401b5e79f66d7037e424f...   \n",
       "7  layout:xla:default:fbaa8bb6a1aed9988281085c910...   \n",
       "\n",
       "                                          TopConfigs  \n",
       "0  730;585;874;498;787;197;709;345;244;232;323;36...  \n",
       "1  93;794;541;178;234;39;853;948;275;357;245;75;1...  \n",
       "2  757;395;337;522;916;178;958;779;179;540;687;12...  \n",
       "3  85;132;93;757;107;163;991;145;850;786;125;51;8...  \n",
       "4  366;245;153;310;711;848;111;313;256;689;232;33...  \n",
       "5  425;411;729;620;367;286;571;788;379;59;899;658...  \n",
       "6  428;310;625;920;731;216;619;217;153;879;52;415...  \n",
       "7  668;224;881;242;222;546;726;924;658;859;984;16...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_configs = []\n",
    "\n",
    "for file in file_preds.keys():\n",
    "    top_configs.append(np.array(file_preds[file]).reshape(-1).argsort())\n",
    "\n",
    "prediction_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"ID\": [\"layout:xla:default:\" + f.split(\".\")[0] for f in list(file_preds.keys())],\n",
    "        \"TopConfigs\": [\";\".join([str(e) for e in top_configs[i]]) for i in range(len(top_configs))],\n",
    "    }\n",
    ")\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(\"outputs_csv/mlp_ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
